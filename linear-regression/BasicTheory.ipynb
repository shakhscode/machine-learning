{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c414d1e-e06e-42a9-9af7-74d25666f648",
   "metadata": {},
   "source": [
    "# Regression \n",
    "A predictive model to establish the relationship between a dependent variable and independent variable(s) to predict continous output for a real given value as input.\n",
    "\n",
    "# Linear Regression \n",
    "When the dependent variable and the independent variable(s) are linearely related then it is called linear regression. \n",
    "In linear regression data is modeled using a linear equation. For example in a dataset with **m** no. of features the output for a particular instance is given by \n",
    "$$ y = w_0+w_1x_1+w_2x_2 +...w_mx_m$$\n",
    "Where $ w_0,w_1,...,w_m $ are randomly introduced coefficients. In statistics these coefficients are calcualted using certain formulae but in Machine learning these are first introduced randomly and then values are updated using an optimization algorithm to get optimal coefficients for the best fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fd0f4e-fd1b-46a2-afe7-b31a7e2c7b73",
   "metadata": {},
   "source": [
    "## Concept of linear regression.\n",
    "In linear regression, we assume that output variable $ y $ is dependent on feature variables $ X $ in a linear manner. The assumption is called linear hypothesis and for a particular data point it is expressed as  $$ h_i(w,x) = w_0x_{0,i}+w_1x_{1,i}+w_2x_{2,i}+.....+ w_mx_{m,i} $$ \n",
    "$$ or, h_i(w,x) = \\sum_{j=0}^{m} w_jx_{j,i} \\ \\ \\ ,\\ Here  \\ x_{j,i} is \\ the \\ i^{th} \\ instance \\ of \\ j^{th} \\ feature.$$\n",
    "for total $ m $ no. of feature.The feature $ x_{0,i} $ is an added bias and $ x_{0,i} = 1 $ for any $ i = 1,2,3,....,n $. Here $ n $ is the total no. of data points in the dataset. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c82f76",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "- $ m = $ total no. of features. And $ j $ is used to iterate for features. $ j = 0,1,2,.....,m $ \n",
    "- The $ 0^{th} $ features is just an added feature with all values = 1, it is also called a bias.\n",
    "- $ n = $ total no. of data points. And $ i $ is used to iterate for data points. $ i = 1,2,3,4......,n $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d3ca56-1d46-436b-9ffa-e7e40e92f33d",
   "metadata": {},
   "source": [
    "### Error and Cost function:\n",
    "Our actual output is $ y_i $ and output accroding to hypothesis is $ h_i $. So error in hypothesis is \n",
    "$$ error = (y_i - h_i) $$ \n",
    "And for all instances total error is given by mean square error (MSE)\n",
    "\n",
    "$$  MSE = \\sum_{i=1}^{n}(y_i-h_i)^2 $$\n",
    "\n",
    "In linear regression optimization goal is to find best coefficients of regression so that this error is minimum. Best values are obtained in an iterative process and this error function determines the computional cost of the overall process. Thus error function is also called cost function which is expressed as \n",
    "$$ J(w) = \\frac{1}{2n}\\sum_{i=1}^{n}(y_i-h_i)^2 $$\n",
    "And optimization or learning objective is \n",
    "$$ minimize \\ J(w) = minimize \\ \\frac{1}{2n}\\sum_{i=1}^{n}(y_i-h_i)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf03665c-333e-4e07-b91e-7b143ecd7634",
   "metadata": {},
   "source": [
    "### Gradient descent \n",
    "> In simple words, a gradient is a measure of how much the output of a function changes if we change the inputs. (By Lex Fridman, MIT)\n",
    "\n",
    "> In mathematical term, gradient descent is an optimization algorithm that numerically finds the local minimum of a multivariable function.\n",
    "\n",
    "In machine learning an iterative gradient descent algorithm is used to minimize the cost function.\n",
    "The partial derivatives \n",
    "$$\\frac{\\partial J(w)}{\\partial w_0},\\frac{\\partial J(w)}{\\partial w_1},...\\frac{\\partial J(w)}{\\partial w_m} $$ \n",
    "are considered as gradients and updated in each iteration using the algorithm\n",
    "$$Repeat \\ until \\ convergence\n",
    "\\left\\{\\begin{matrix}\n",
    "w_j := w_j - lr \\times \\frac{\\partial J(w)}{\\partial w_j}\n",
    "\\end{matrix}\\right.\n",
    "\\\\\n",
    "for \\ j=0,1,2,3,...,m\n",
    " $$\n",
    "Here $ lr = learning \\ rate $. \n",
    "\n",
    "But, when implemented in programming, then $ w_j $ will be updated in each iteration as \n",
    "$$ Repeat \\ until \\ convergence\n",
    "\\left\\{\\begin{matrix}\n",
    "w_j := w_j - lr \\times \\frac{1}{n} \\sum_{i=1}^{n} (h_i - y_i)^2 x_{j,i}\n",
    "\\end{matrix}\\right.\n",
    "$$\n",
    "\n",
    "For example $ w_0, w_1 $ will be updated in each iteration as\n",
    "$$\n",
    "temp0= w_0 - lr \\times \\frac{1}{n} \\sum_{i=1}^{n} (h_i - y_i)^2 x_{0,i}\n",
    "$$\n",
    "\n",
    "$$ Or, temp0= w_0 - lr \\times \\frac{1}{n} \\sum_{i=1}^{n} (h_i - y_i)^2, as \\ x_{0,i}= 1\n",
    "$$\n",
    "\n",
    "And,\n",
    "$$ temp1= w_1 - lr \\times \\frac{1}{n} \\sum_{i=1}^{n} (h_i - y_i)^2 x_{1,i} $$\n",
    "\n",
    "$$\n",
    "\\\\ w_0 = temp0 \\\\ w_1 = temp1\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6054345",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "Once the coefficients with minimum error are found, then these coefficients are used to predict $ \\hat{y}_i $ as future value of  $ x_i $.\n",
    "\n",
    "So $ \\hat{y}_i $ is the predicted output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f79661e-167f-4d3d-b500-b6f79441a824",
   "metadata": {},
   "source": [
    "## Linear regression in Vectorized form\n",
    "\n",
    "If there are total **m no. of features and n no. of instances** then the feature matrix $ X $ is given by \n",
    "$$X= \\begin{bmatrix}\n",
    "x_{0,1}=1 & x_{1,1} & x_{2,1} & ... & ... & x_{m,1}\\\\ \n",
    "x_{0,2}=1 & x_{1,2} & x_{2,2} & ... & ... & x_{m,2} \\\\ \n",
    "x_{0,3}=1 & x_{1,3} & x_{2,3} & ... & ... & x_{m,3}\\\\ \n",
    " ... & ...& ... & ... & ... & ...\\\\\n",
    "... & ...& ... & ... & ... & ...\\\\\n",
    "x_{0,n}=1 & x_{1,n} & x_{2,n} & ... & ... & x_{m,n}\n",
    "\\end{bmatrix}_{n\\times(m+1)}\n",
    "$$\n",
    "$$ x_{j,i} = j^{th} \\ feature, \\ i^{th} \\ instance$$\n",
    "In the feature matrix a cloumn represents a feature and a row represents a data point.\n",
    "\n",
    "The coefficient matrix is introduced as \n",
    "$$W= \\begin{bmatrix}\n",
    "w_0\\\\ w_1\n",
    "\\\\ w_2\n",
    "\\\\ ...\n",
    "\\\\ ...\n",
    "\\\\ \n",
    "w_m\n",
    "\\end{bmatrix}_{(m+1)\\times 1}$$\n",
    "\n",
    "So hypothesis is given by \n",
    "$$ h(w) = XW\\\\\n",
    "h(w)=\\begin{bmatrix}\n",
    "x_{0,1}=1 & x_{1,1} & x_{2,1} & ... & ... & x_{m,1}\\\\ \n",
    "x_{0,2}=1 & x_{1,2} & x_{2,2} & ... & ... & x_{m,2} \\\\ \n",
    "x_{0,3}=1 & x_{1,3} & x_{2,3} & ... & ... & x_{m,3}\\\\ \n",
    " ... & ...& ... & ... & ... & ...\\\\\n",
    "... & ...& ... & ... & ... & ...\\\\\n",
    "x_{0,n}=1 & x_{1,n} & x_{2,n} & ... & ... & x_{m,n}\n",
    "\\end{bmatrix}_{n\\times(m+1)} \\times \n",
    "\\begin{bmatrix}\n",
    "w_0\\\\ w_1\n",
    "\\\\ w_2\n",
    "\\\\ ...\n",
    "\\\\ ...\n",
    "\\\\ \n",
    "w_m\n",
    "\\end{bmatrix}_{(m+1)\\times 1}\n",
    "$$\n",
    "$$ or, h(w)= \\begin{bmatrix}\n",
    "h_1\\\\ h_2\n",
    "\\\\ h_3\n",
    "\\\\ ...\n",
    "\\\\ ...\n",
    "\\\\ h_n\n",
    "\\end{bmatrix}_{n\\times 1} $$\n",
    "\n",
    "If **y** is the output vector then error is given by\n",
    "\n",
    "$$ \n",
    "error = \n",
    "\\begin{bmatrix}\n",
    "h_1\\\\ h_2\n",
    "\\\\ h_3\n",
    "\\\\ ...\n",
    "\\\\ ...\n",
    "\\\\ h_n\n",
    "\\end{bmatrix}-\n",
    "\\begin{bmatrix}\n",
    "y_1\\\\ y_2\n",
    "\\\\ y_3\n",
    "\\\\ ...\n",
    "\\\\ ...\n",
    "\\\\ y_n\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "And error function / cost function in vectorized form is given by \n",
    "$$ J(W) = \\frac{1}{2n}[(XW-y)^T(XW-y)]$$\n",
    "\n",
    "In vectorized form gradient descent is given by (The whole coefficient vector can be updated at once)\n",
    "$$ W = W - \\frac{lr}{n}X^T(XW-y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85f5663-9d5d-4cf1-bc7d-fdde60611cf7",
   "metadata": {},
   "source": [
    "### Normal Equation\n",
    "Normal equation is another way to find optimal coefficients of regression without using gradient descent algorithm.\n",
    "Using normal eqation the optimal coefficient is given by \n",
    "$$ W = (X^T X)^{-1}X^Ty $$\n",
    "\n",
    "### Gradient descent vs Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a27f385",
   "metadata": {},
   "source": [
    "Gradient descent | Normal Equation\n",
    "-----------------|-----------------\n",
    "Need to choose $ \\alpha $ value. | No need to choose $ \\alpha $\n",
    "Involves iteration | No iteration is required.\n",
    "$ O(Kn^2) $ | $ O(n^3), need to calculate invers of X^TX $\n",
    "Works well when n is large. | When n is large, processing becomes slow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8381890-89f4-401a-bfb2-4a3714ecc0c9",
   "metadata": {},
   "source": [
    "### Measures to judge a linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62d1065",
   "metadata": {},
   "source": [
    "### R Square \n",
    "R squared value is a statistical measure of how close the data are to the fitted regression line. Also known as coefficient of determination.\n",
    "\n",
    "$$ R^2 = 1 - \\frac{\\sum(y_i -\\hat{y}_i)^2 }{\\sum(y_i -\\bar{y})^2} $$\n",
    "\n",
    "\n",
    "- The difference between actual $ y_i $ and predicted $ \\hat{y}_i $ are called residuals.\n",
    "- Usually, $ 0 \\leq R^2 \\leq 1 $\n",
    "- Usually, the larger the R2, the better the regression model.\n",
    "\n",
    "\n",
    "\n",
    "#### Limitations of $ R^2 $\n",
    "- We cannot use R-squared to determine whether the coefficient estimates and predictions are biased, which is why you must assess the residual plots.\n",
    "-  it does not take into consideration of overfitting problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e35ba15",
   "metadata": {},
   "source": [
    "#### Adjusted $ R^2 $\n",
    "\n",
    "![](AdjustedRSquare.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a831c1",
   "metadata": {},
   "source": [
    "### MSE: Mean Square Error\n",
    "$$  MSE = \\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3ef193",
   "metadata": {},
   "source": [
    "### MAE:Mean Absolute error\n",
    "$$  MSE = \\sum_{i=1}^{n}|y_i-\\hat{y}_i|^2 $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6 (default, Jan  8 2020, 19:59:22) \n[GCC 7.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "1448b48b023bcc9c3d4a79e814720a10ca6d4244f75e0f7ce4af58f96ba2b7d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
